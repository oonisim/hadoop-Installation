#--------------------------------------------------------------------------------
# Local hadoop manifest folder
# Hadoop module is responsible to provide the hadoop cluster information e.g.
# hadoop Name node hostname/IP, hadoop configuration files, etc, so that 
# services e.g. Spark module can use those information to configure.
# 
# Those information is store in files under the manifest holder.
#
# 07DEC2019: Stop using HADOOP_ARTEFACT_DIR
# If Hadoop information is required, get from hadoop/etc or runtime.
#--------------------------------------------------------------------------------
#HADOOP_ARTEFACT_DIR: "{{ lookup('env','INSTALL_HOME') }}//artefacts/hadoop"

#--------------------------------------------------------------------------------
# Hadoop installation package
#--------------------------------------------------------------------------------
HADOOP_VERSION: 3.2.2
HADOOP_PACKAGE: "hadoop-{{ HADOOP_VERSION }}.tar.gz"
HADOOP_DOWNLOAD_URL: "https://archive.apache.org/dist/hadoop/common/hadoop-{{ HADOOP_VERSION }}/{{ HADOOP_PACKAGE }}"
HADOOP_PACKAGE_CHECKSUM: "sha512:054753301927d31a69b80be3e754fd330312f0b1047bcfa4ab978cdce18319ed912983e6022744d8f0c8765b98c87256eb1c3017979db1341d583d2cee22d029"
HADOOP_DOWNLOAD_DIR: "~{{ HADOOP_ADMIN }}/downloads"

#--------------------------------------------------------------------------------
# Administration account
#--------------------------------------------------------------------------------
HADOOP_ADMIN : hadoop
HADOOP_GROUP: hadoop
HADOOP_ADMIN_PASSWD:

#--------------------------------------------------------------------------------
# Home
#--------------------------------------------------------------------------------
HADOOP_BASE: "/opt/hadoop"
HADOOP_HOME: "{{ HADOOP_BASE }}/hadoop-{{ HADOOP_VERSION }}"
HADOOP_CONF_DIR: "{{ HADOOP_HOME }}/etc/hadoop"

#--------------------------------------------------------------------------------
# Core
# https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml
#--------------------------------------------------------------------------------
# hadoop.tmp.dir
HADOOP_CORE_TMP_DIR: "/tmp/hadoop-{{ HADOOP_ADMIN }}"

#--------------------------------------------------------------------------------
# Name Node configurations
#--------------------------------------------------------------------------------
# fs.defaultFS
HADOOP_NN_HOSTNAME: "{{ lookup('env','HADOOP_NN_HOSTNAME') }}"
HADOOP_NN_PORT: 8020
HADOOP_WORKERS: "{{ lookup('env','HADOOP_WORKERS') }}" 

#--------------------------------------------------------------------------------
# YARN RM configurations
#--------------------------------------------------------------------------------
HADOOP_RM_HOSTNAME: "{{ lookup('env','YARN_RM_HOSTNAME') }}"
HADOOP_RM_PORT: 8088

#--------------------------------------------------------------------------------
# HDFS configurations
# https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml
#--------------------------------------------------------------------------------
HADOOP_DFS_REPLICATION: 1
HADOOP_DFS_BLOCK_SIZE: 256m
# io.file.buffer.size (Hadoop uses buffer size of 4KB by default for its I/O operations)
# Can increase it to 128K in order to get good performance
HADOOP_IO_FILE_BUFFER_SIZE: 131072
# dfs.client.read.shortcircuit (Read data block directly from the disk on the node if it is on the same node)
# Needs native library and dfs.domain.socket.path setting is required.
# https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ShortCircuitLocalReads.html
# https://www.cloudera.com/documentation/enterprise/5-12-x/topics/admin_hdfs_short_circuit_reads.html
HADOOP_DFS_CLIENT_READ_SHORTCIRCUIT: false
# dfs.domain.socket.path
HADOOP_DFS_DOMAIN_SOCKET_PATH: /var/lib/hadoop-hdfs/dn_socket
# dfs.datanode.data.dir
HADOOP_DFS_DATA_DIR: "/home/{{ HADOOP_ADMIN }}/dfs/data"



#--------------------------------------------------------------------------------
# YARN configurations
# https://hadoop.apache.org/docs/r2.7.6/hadoop-yarn/hadoop-yarn-common/yarn-default.xml
#--------------------------------------------------------------------------------
# yarn.resourcemanager.hostname
YARN_RM_HOSTNAME:  "{{ lookup('env','YARN_RM_HOSTNAME') }}"
YARN_RM_URL: "http://{{ YARN_RM_HOSTNAME }}:8088"
# yarn.nodemanager.resource.cpu-vcores (The number of cores that a node manager can allocate to containers)
# It should be set to the total number of cores on the machine, minus a core for each daemon process running on the machine (datanode, node manager, and any other long-running processes).
YARN_NM_CPU_VCORES: 4
# yarn.nodemanager.resource.memory-mb (Total amount of physical memory (RAM) for Containers on worker node.)
# Set this property= Total RAM – (RAM for OS + Hadoop Daemons + Other services)
YARN_NM_MEMORY_MB: 12288
# yarn.nodemanager.aux-services
YARN_NM_AUX_SERVICES: mapreduce_shuffle
# yarn.nodemanager.aux-services.mapreduce_shuffle.class
YARN_NM_AUX_SERVICES_SHUFFLE_CLASS: org.apache.hadoop.mapred.ShuffleHandler
# yarn.nodemanager.vmem-check-enabled
# To avoid Container running beyond virtual memory limits on CentOS/RHEL, set to false.
# http://blog.cloudera.com/blog/2014/04/apache-hadoop-yarn-avoiding-6-time-consuming-gotchas/
# https://issues.apache.org/jira/browse/YARN-4714
YARN_NM_VMEM_CHECK_ENABLED: false

#--------------------------------------------------------------------------------
# Map Reduce configurations
#--------------------------------------------------------------------------------
# mapreduce.map.memory.mb
YARN_MR_MAP_MEMORY_MB: 2048
# mapreduce.reduce.memory.mb
YARN_MR_REDUCE_MEMORY_MB: 2048
# mapreduce.map.java.opts (must be < YARN_MR_MAP_MEMORY_MB)
# https://stackoverflow.com/questions/24070557
# http://doc.mapr.com/display/MapR/mapred-site.xml
YARN_MR_MAP_JAVA_OPTS: -Xmx1024M
# mapreduce.reduce.java.opts (must be < YARN_MR_REDUCE_MEMORY_MB)
YARN_MR_REDUCE_JAVA_OPTS: -Xmx1024M